from __future__ import unicode_literals
import json
from hazm import *


docsContents = []
docsTitles = []
docsUrls = []
positionalIndex = {}
stopWordsList = ["با", "و", "در", "ولی", "اما", "نیز", "اگر", "که", "مگر", "از", "بر", "تا", "بی", "الا", "غیر", ".",",", "،", ".", "/", "را", "مانند", "جزو", ":", "به", "؛", "اینجا", "گردد", "این", "است", "زاده", "لذا", "می‌یابد", "دوباره", "،", "اغلب", "جمعی", "گاه", "خاطرنشان", "پر", "یافته_است", "نیستند", "برای", "آن", "یک", "خود", "کرد", "هم", "گفت", "می‌شود", "وی", "شد", "دارد", "ما", "یا", "شده", "باید", "هر", "آنها", "بود", "او", "دیگر", "دو", "مورد", "می‌کند", "شود", "کند", "وجود", "بین", "پیش", "شده_است", "پس", "نظر", "همه", "یکی", "حال", "هستند", "من", "کنند", "نیست", "باشد", "چه", "می", "بخش", "می‌کنند", "همین", "افزود", "هایی", "دارند", "راه", "همچنین", "روی", "داد", "بیشتر", "بسیار", "سه", "داشت", "چند", "سوی", "تنها", "هیچ", "میان", "اینکه", "شدن", "بعد", "جدید", "حتی", "کردن", "برخی", "کردند", "می‌دهد", "اول", "نه", "کرده_است", "نسبت", "بیش", "شما", "چنین", "طور", "افراد", "تمام", "درباره", "بار", "بسیاری", "می‌تواند", "کرده", "چون", "ندارد",  "دوم", "بزرگ", "طی", "حدود", "همان", "بدون", "البته", "آنان", "می‌گوید", "دیگری", "خواهد_شد", "کنیم", "قابل", "یعنی", "رشد", "می‌توان", "وارد", "کل", "ویژه", "قبل", "براساس", "نیاز", "گذاری", "هنوز", "لازم", "سازی", "بوده_است", "چرا", "می‌شوند", "وقتی", "گرفت", "کم", "جای", "حالی", "تغییر", "پیدا", "اکنون", "تحت", "باعث", "مدت", "فقط", "زیادی", "تعداد", "آیا", "بیان", "رو", "شدند", "عدم", "کرده_اند", "بودن", "نوع", "بلکه", "جاری", "دهد", "برابر", "مهم", "بوده", "اخیر", "مربوط", "امر", "زیر", "گیری", "شاید", "خصوص", "آقای", "اثر", "کننده", "بودند", "فکر", "کنار", "اولین", "سوم", "سایر", "کنید", "ضمن", "باز", "می‌گیرد", "ممکن", "حل", "دارای", "پی", "مثل", "می‌رسد", "اجرا", "دور", "منظور", "کسی", "موجب", "طول", "امکان", "آنچه", "تعیین", "گفته", "شوند", "جمع", "خیلی", "علاوه", "گونه", "تاکنون", "رسید", "ساله", "گرفته", "شده_اند", "علت", "چهار", "داشته_باشد", "خواهد_بود", "طرف", "تهیه", "تبدیل", "مناسب", "زیرا", "مشخص", "می‌توانند", "نزدیک", "جریان", "روند", "بنابراین", "می‌دهند", "یافت", "نخستین", "بالا", "پنج", "ریزی", "عالی", "چیزی", "نخست", "بیشتری", "ترتیب", "شده_بود", "خاص", "خوبی", "خوب", "شروع", "فرد", "کامل", "می‌رود", "دهند", "آخرین", "دادن", "جدی", "بهترین", "شامل", "گیرد", "بخشی", "باشند", "تمامی", "بهتر", "داده_است", "حد", "نبود", "کسانی", "می‌کرد", "داریم", "علیه", "می‌باشد", "دانست", "ناشی", "داشتند", "دهه", "می‌شد", "ایشان", "آنجا", "گرفته_است", "دچار", "می‌آید", "لحاظ", "آنکه", "داده", "بعضی", "هستیم", "اند", "برداری", "نباید", "می‌کنیم", "نشست", "سهم", "همیشه", "آمد", "اش", "وگو", "می‌کنم", "حداقل", "طبق", "جا", "خواهد_کرد", "نوعی", "چگونه", "رفت", "هنگام", "فوق", "روش", "ندارند", "سعی", "بندی", "شمار", "کلی", "کافی", "مواجه", "همچنان", "زیاد", "سمت", "کوچک", "داشته_است", "چیز", "پشت", "آورد", "حالا", "روبه", "سال‌های", "دادند", "می‌کردند", "عهده", "نیمه", "جایی", "دیگران", "سی", "بروز", "یکدیگر", "آمده_است", "جز", "کنم", "سپس", "کنندگان", "خودش", "همواره", "یافته", "شان", "صرف", "نمی‌شود", "رسیدن", "چهارم", "یابد", "متر", "ساز", "داشته", "کرده_بود", "باره", "نحوه", "کردم", "تو", "شخصی", "داشته_باشند", "محسوب", "پخش", "کمی", "متفاوت", "سراسر", "کاملا", "داشتن", "نظیر", "آمده", "گروهی", "فردی", "ع", "همچون", "خطر", "خویش", "کدام", "دسته", "سبب", "عین", "آوری", "متاسفانه", "بیرون", "دار", "ابتدا", "شش", "افرادی", "می‌گویند", "سالهای", "درون"]
def read():
    f = open('../IR_data_news_12k.json', encoding='utf8')
    data = json.load(f)
    for i in data:
        docsTitles.append(data[i]["title"])
        docsContents.append(data[i]["content"])
        docsUrls.append(data[i]["url"])
    f.close()
    
def normalize(data, i):
    normalizer = Normalizer()
    data = normalizer.normalize(data)
    tokenizedData = word_tokenize(data)
    stemmer = Stemmer()
    for j in range(len(tokenizedData)):
        tokenizedData[j] = stemmer.stem(tokenizedData[j])
    for i in tokenizedData:
        if i in stopWordsList:
            tokenizedData.remove(i)
    index(tokenizedData, i)
    
def index(tokenizedData, i):
    for j in range(len(tokenizedData)):
        data = tokenizedData[j]
        if data in positionalIndex:
            positionalIndex[data][0] = positionalIndex[data][0] + 1
            if i in positionalIndex[data][1]:
                positionalIndex[data][1][i].append(j)
            else:
                positionalIndex[data][1][i] = [j]
        else:
            positionalIndex[data] = []
            positionalIndex[data].append(1)
            positionalIndex[data].append({})
            positionalIndex[data][1][i] = [j]


read()
for i in range(len(docsContents)):
    normalize(docsContents[i], i)
print(positionalIndex)